# Technial Task Overview
## Environmental Sound Classification Pipeline
A lightweight, edgeâ€‘deployable ML system for ESCâ€‘50 audio classification

This repository implements a complete endâ€‘toâ€‘end machine learning pipeline for environmental sound classification using the ESCâ€‘50 dataset. The focus of the project is to design a compact, efficient, and edgeâ€‘friendly audio model capable of running on lowâ€‘power devices while maintaining strong classification performance.

The pipeline includes:
- Audio preprocessing
- MFCC feature extraction
- Stratified train/test splitting
- A lightweight CNN architecture
- Training & tuning strategy
- Example training script
- Edge deployment considerations

## Task Setup Instructions
1. Clone the repository
```bash
git clone <your-repo-url>
cd <your-repo-name>
```
2. Create and activate a virtual environment
```bash
python3 -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Windows
```
3. Install dependencies
```bash
pip install -r requirements.txt
```
4. Your folder structure should look like:
```Code
data/
 â”œâ”€â”€ raw/
â”‚     â”œâ”€â”€ audio/
 â”‚    â””â”€â”€ meta/esc50.csv
 â””â”€â”€ processed/
```
ESC-50 data already downloaded 
5. Run preprocessing + feature extraction
```bash
python src/preprocessing.py
python src/feature_extraction.py
```
This generates MFCC feature files in:
```Code
data/processed/features/
```
6. (Optional) Test the model with dummy + real MFCCs
```bash
python src/train_dummy.py
```

## Full ML Pipeline Overview
```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ENDâ€‘TOâ€‘END ML PIPELINE                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                          Raw Audio Files (.wav)
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         1. Preprocessing Stage                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ Load audio (librosa)
   â€¢ Resample to 44.1 kHz
   â€¢ Convert to mono
   â€¢ Trim silence
   â€¢ Normalise amplitude
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     2. Feature Extraction (MFCCs)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ Compute MFCCs (13 coefficients)
   â€¢ Shape: (n_mfcc, time_frames)
   â€¢ Save as .npy files in data/processed/features/
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     3. Load Features & Metadata                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ load_features.py:
       - Load MFCC .npy files
       - Load esc50.csv labels
       - Build X (features) and y (labels)
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     4. Train/Test Split (Stratified)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ sklearn.train_test_split()
   â€¢ stratify=y to preserve class balance
   â€¢ Avoids:
       - data leakage
       - temporal bias
       - class imbalance issues
       - overfitting to recording conditions
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         5. Model Architecture                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ AudioCNN (lightweight 2â€‘layer CNN)
   â€¢ Input: (batch, 1, 13, time_frames)
   â€¢ Conv â†’ ReLU â†’ Pool â†’ Conv â†’ ReLU â†’ Pool â†’ Dropout â†’ FC â†’ Output
   â€¢ Designed for edge deployment (small, fast, quantisable)
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         6. Training & Tuning                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ Loss: CrossEntropy
   â€¢ Optimiser: Adam
   â€¢ Regularisation: dropout, early stopping
   â€¢ Data augmentation (noise, pitch shift, stretch)
   â€¢ Hyperparameter tuning (LR, filters, dropout)
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           7. Evaluation                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ Accuracy
   â€¢ Confusion matrix
   â€¢ Perâ€‘class F1
   â€¢ Validate generalisation across 50 ESCâ€‘50 classes
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        8. Edge Deployment Prep                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â€¢ Quantisation (8â€‘bit)
   â€¢ Pruning
   â€¢ Export to ONNX
   â€¢ Deploy via TensorRT / ONNX Runtime Mobile

```


# Technical Assesment - detailed reasoning/answers 
## Section 1: Loading & Exploring
This is your exploratory data analysis (EDA) stage:

Load audio files with librosa or torchaudio.

Inspect metadata (esc50.csv) â†’ check class distribution, folds, clip durations.

Visualize:

Waveforms (time domain).

Spectrograms (frequency domain).

Histograms of clip lengths or class counts.

Document issues:

Are some clips noisy, clipped, or silent?

Are classes imbalanced (ESCâ€‘50 has 40 clips per class, so itâ€™s balanced â€” but note if you subset)?
Any sample rate inconsistencies? (ESCâ€‘50 is standardized at 44.1 kHz, so you can mention that).

##âš™ï¸ Preprocessing â†’ ML Features
This is the feature engineering stage:

Convert raw .wav â†’ MFCCs (or Mel spectrograms).

Normalize features (perâ€‘clip mean/variance).

Pad or truncate to fixed length (ESCâ€‘50 clips are all 5s, so youâ€™re safe).

Save features into data/processed/ for reuse.

ðŸ‘‰ For CNNs: treat MFCCs or spectrograms as 2D â€œimagesâ€ (coefficients Ã— time). ðŸ‘‰ For RNNs/Transformers: treat MFCCs as sequential feature vectors over time.

### preprocessing script
âœ… What this script does
Loads metadata (esc50.csv) and prints dataset stats.

Explores: class distribution, clip durations, sample rates.

Loads audio: returns waveform + sample rate.

Extracts features: spectrograms and MFCCs.

Documents: ESCâ€‘50 is balanced (40 clips per class), clips are 5s long, sample rate is 44.1 kHz.



### feature extraction script
ðŸ”Ž What the script produces
Spectrograms (dB) â†’ 2D arrays of shape (freq_bins, time_frames)

MFCCs â†’ 2D arrays of shape (n_mfcc, time_frames)

Labels â†’ the class name (e.g. "dog", "rain", "engine") for each clip

These are saved both:

Individually (clipname_spec.npy, clipname_mfcc.npy)

As combined arrays (all_specs.npy, all_mfccs.npy, labels.npy)

ðŸ§‘â€ðŸ’» How this becomes a training/test dataset
Youâ€™ll load all_mfccs.npy (or all_specs.npy) and labels.npy.

Each entry is a 2D feature matrix for one clip.

You then split them into train/test sets (e.g. 80/20 split).

Depending on the model, you might:

Flatten the 2D arrays into 1D vectors (for classical ML like SVM, RandomForest).

Keep them as 2D â€œimagesâ€ (for CNNs).

Pad/truncate to a fixed length if needed.



## Section 2: Data Splitting Strategy
### Load features script overview
Purpose is to reduce RAM required to create merge features file and perform data splitting strategy.
âœ… What this does
1. Reads metadata (esc50.csv) to know which clips exist.
2. Loads the corresponding _mfcc.npy or _spec.npy file for each clip generated by feature_extraction.py
3. Builds arrays X (features) and y (labels) by using the metadata in esc50.csv to map them to the feature arrays.
  - X -> list/array of feature matrices
  - Y -> list/array of class labels
4. Splits into train/test sets with stratification.
  - This is done by the train_test_split() from the python pkg sklearn.model_selection and ensures:
    - balanced classes 
    - no leakage
    - no bias 
What its actually doing:
You provide it with X and Y arrays its returns X and Y train and test arrays whereby:
- test_size=0.2 splits train and test 80/20
- random_state=42 shuffles dataset to prevent ordering bias and ensure they are representative. 42 choosen for reproducibility and widely used, any integer would work.
- stratify=y ensure class proportions in train and test match original dataset i.e. no over- or under- representation in either split
- the above utility ensures no sample appears in both sets and no lable or feature leaks across the two sets.

### Explanation to explain the following:
#### 1. Avoiding Data Leakage
What leakage means in audio:  
This happens when information from the test set â€œbleedsâ€ into the training set â€” for example, if the same recording session appears in both sets.ESCâ€‘50 is a benchmark dataset where each clip is an independent 5â€‘second recording. There are no repeated clips, no overlapping segments, and no multiâ€‘segment recordings.

The strategy is then as follows:
 - Splitting is done after feature extraction.
 - train_test_split(..., stratify=y) is used to ensure labels are separated cleanly.
 - No clip appears in both sets.
 - No metadata or feature normalization is computed using test data.

More information: 
Leakages can occur in the following scenarios:
- poor splitting of dataset
- shared preprocessing 
- overlapping samples
Repercussion of this are the following:
- inflated model performance due to model seeing data already when trained and then being tested on it.
- poor generalisation - doesn't learn off features its been supervised to learn off 
- contamination of accuracy, CM, loss curves etc,. 
- overfitting to recording conditions with the audio rather than actual sound class (similiar to poor generalisation)

#### 2. Avoiding Temporal Bias
What temporal bias means:  
In timeâ€‘series datasets, you must ensure training data occurs before test data. ESCâ€‘50 is not a timeâ€‘series dataset.
Clips are independent and not ordered in time. Therefore:
- Random stratified split is appropriate.
- No temporal ordering exists to preserve.

Repercussion of temporal bias and what I would do if present in a dataset:
Concept:
Occurs if the order of the data in time influences the model in a misleading way because if you randomly shuffle and split this data you can train data that comes later than the data you test with.
Reprecussions:
- inflated model performance due to indirectly seeing future data 
- poor generalisation to new data as it has learned pattern based on the future data portion if the dataset
- temporal drift in audio occurs due to enviornmental changes, new noise, seasons etc,. and if models mixed time periods incorrectly the relationships between the features can be learnt incorrectly.
- overfitting if acoustic features in the training audio comes from a time window completely different from the test. Therefore you could fail to recognise a class in a future deployment.

What I would have done given my dataset did not have this issue would be:
- use chronological splitting 
- group recordings by session/device/location 
- utilised the following utilites in sklearn (TimeSeriesSplit, GroupShuffleSplit and GroupKFold) to prevent temporal overlap
- Disable shuffling (unlike what I done with random_state)

3. Avoiding Class Imbalance Issues
ESCâ€‘50 dataset is balanced with 50 classes, 40 clips per class and 2000 total clips.

Approach used:
- stratify=y is used in train_test_split, which ensures:
  - Each class appears in train and test in the same proportion.
  - No class is overâ€‘ or underâ€‘represented.

Repercussions of class imbalance:
- Majority class dominance (FP)
- Minority class misclassified (FN)
- Skewed loss function as model optimising for the wrong objective 
- Metrics are misleading (Precision, recall, F1 score etc,.)
- Overfitting for majority class patterns 

Approaches to reduce class imbalance:
- augmentation (pitch shift, add audio artifacts)
- synthetic samples 
- unsampled majority class (adjust model architecture size accordingly)
- class weighted loss function 
- collect more data

4. Avoiding Overfitting to Recording Conditions / Equipment
ESCâ€‘50 recordings come from many sources e.g. microphones, environments, and noise levels.

Approach used:
- Random stratified split ensures:
  - Each classâ€™s variability (different microphones, environments) is represented in both train and test.
  - You do not group by recording session (ESCâ€‘50 doesnâ€™t provide session IDs anyway).
Therefore -> split distributes recording conditions across train/test, reducing overfitting risk.
- Used MFCCs feature (domain-invariant) i.e. compress spectral info and reduce sensitivity to equiptment frequency artifacts

Repercussions:
- Similiar outcomes as above issues in terms of model performances as if class representation over recording conditions/equiptment is uneven it increases potential of model learning incorrect audio features over the target features. Therefore, when using the model on new devices or environments the model under performs.

Further improvements beyond representative classes over different instruments:
- Collate contextual metadata (device ID, location, seesion ID any other relevant deployment information)
- Reduce influence of equiptment by normalising volume, amplitudes etc,. 
- Perform cross-device validation


## Section 3: Model Architecture Selection 

### Model Architecture Section 
Model Architecture
For this project, I selected a lightweight Convolutional Neural Network (CNN) operating on MFCC features. This architecture is intentionally small and efficient, making it suitable for edge deployment on devices such as Raspberry Pi, Jetson Nano, or ARMâ€‘based embedded systems.

Why MFCCs?
MFCCs provide a compact representation of audio (13 Ã— time frames), dramatically reducing input size compared to full spectrograms. This lowers computational cost and memory usage while preserving the key frequency characteristics needed for classification.

Why a Small CNN?
A shallow CNN is ideal for edge compute because:
  - It captures local timeâ€“frequency patterns in MFCCs
  - It has a very small parameter count
  - It runs efficiently on lowâ€‘power hardware
  - It is easy to quantise and prune after training
This makes it a strong starting point for realâ€‘time acoustic classification.

### CNN Architecture Diagram

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        AudioCNN Model                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input: MFCC tensor (batch, 1, 13, T)
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Conv2D (1â†’16) â”‚ 3Ã—3, padding=1
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   ReLU         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ MaxPool2D 2Ã—2  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Conv2D (16â†’32) â”‚ 3Ã—3, padding=1
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     ReLU       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ MaxPool2D 2Ã—2  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Dropout 0.3  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Flatten      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Fully Connected (â†’ 64 units) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Fully Connected (â†’ classes)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
              Output
```

### Training and Tuning Strategy 
This section explains how I would train, validate, and tune the lightweight CNN model designed for MFCCâ€‘based acoustic classification. The goal is to achieve strong generalisation while keeping the model small and efficient enough for edge deployment.

#### Training Approach
1. Input Features
The model is trained on MFCC features extracted from each audio clip:
  - 13 MFCC coefficients
  - ~100â€“200 time frames (depending on hop length)
  - Input shape to the model -> (batch_size, 1, 13, time_frames)

MFCCs are chosen because they are compact, robust, and computationally inexpensive â€” ideal for edge devices.

2. Training Loop
The model uses a standard supervised learning pipeline:
 - Loss function: Crossâ€‘Entropy Loss
 - Optimiser: Adam
 - Initial learning rate: 1eâ€‘3
 - Batch size: 32
 - Epochs: 20â€“30 (with early stopping)

This setup provides a good balance between stability and speed, especially for small CNNs.

3. Regularisation
To prevent overfitting, especially given the small model size, I would apply:
- Dropout (0.3) â€” already included in the architecture
- Early stopping â€” stop training when validation loss stops improving
- Data augmentation, such as:
  - background noise injection
  - pitch shifting
  - time stretching
  - random gain changes

These augmentations help the model generalise to different recording conditions and environments.

#### Hyperparameter Tuning Strategy
Although the model is intentionally lightweight, tuning still plays an important role. I would use a small, targeted grid search or Bayesian optimisation over:

Model hyperparameters
  - Number of convolutional filters (e.g., 16/32 â†’ 32/64)
  - Kernel sizes (3Ã—3 vs 5Ã—5)
  - Dropout rate (0.2â€“0.5)
  - Size of the fully connected layer (32â€“128 units)

Training hyperparameters
  - Learning rate (1eâ€‘2 â†’ 1eâ€‘4)
  - Batch size (16, 32, 64)
  - Weight decay (0â€“1eâ€‘4)

Data augmentation parameters
  - Noise level
  - Pitch shift range
  - Time stretch factor
Because the model is small, tuning is fast and inexpensive.

### Computational Requirements
#### Model Training 
The model is small enough to train on:
  - A standard laptop CPU
  - Any modern GPU (optional)
  - Cloud compute (if desired)

Training time is typically minutes, not hours.

#### Model Deployment
The architecture is designed for edge compute:
  - <1 MB model size after quantisation
  - <5 ms inference on ARM CPUs
Compatible with:
  - ONNX Runtime Mobile
  - TensorRT (convert ONNX to TFRT)
  - PyTorch Mobile
Making it suitable for embedded audio applications such as:
  - IoT sensors
  - Environmental monitoring nodes 
Deployed on microcrontrollers, jetson nanos or Raspberry Pi's.

### Dummy Training with Real MFCC 
Demonstrating chosen model architecture matches the feature representation. To satisfy this, I created a small script (train_dummy.py) that:
- Loads a real MFCC from the processed dataset
- Prints its shape
- Passes it through the CNN
- Runs a single optimisation step
- Also runs a dummy batch example for completeness
This confirms that the model accepts MFCC inputs shaped (batch, 1, 13, time_frames) and that the forward/backward passes work correctly.

Expected output 
```text
Real MFCC shape: torch.Size([1, 1, 13, 173])
Output shape (real MFCC): torch.Size([1, 50])
Loss (real MFCC): 3.91
Dummy training script completed successfully.
```

## Section 4: Visualisation
Web Application Features included
File selector for .wav files in data/raw/.

Waveform visualization.

Spectrogram (STFT in dB).

MFCC feature visualization.

Audio playback in browser.


Raw audio (.wav) â†’ load with librosa or torchaudio.

Feature extraction â†’ compute MFCCs (or spectrograms).

Dataset pairing â†’ each MFCC array is paired with its label (e.g. class ID).

Data split â†’ train/validation/test sets (careful to avoid leakage).

Model input â†’ MFCCs are fed into your CNN/RNN/Transformer as tensors.

Training â†’ model learns to map MFCC patterns â†’ labels.

Evaluation â†’ accuracy, confusion matrix, etc.


### visualisation script 
âœ… What this gives you
Fileâ€‘level exploration: Waveform, spectrogram, MFCC plots.

Datasetâ€‘level exploration: Class distribution and duration histograms.

Reusable functions: Can be imported into app.py for interactive visualization.

### app script 
âœ… Whatâ€™s this gives you
Uses metadata (esc50.csv) to list files instead of scanning the folder.

Imports functions from preprocessing.py and visualization.py to keep code modular.

Adds dataset-level plots: class distribution and clip duration histograms.

Interactive file selector: lets you pick a clip, play it, and see waveform, spectrogram, and MFCCs.

Random Sample button: Picks a random file from the dataset when clicked.

Keeps the dropdown for manual selection, but adds a quick way to explore.

Displays the filename above the audio player so you know what youâ€™re listening to.

To run app 
> streamlit run src/app.py
in browser run:
http://localhost:8501